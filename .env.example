# Discord Bot Configuration
# Copy this file to .env and fill in your actual values
# NEVER commit .env to git - it contains secrets!

# Discord Bot Token (from https://discord.com/developers/applications)
DISCORD_TOKEN=your_discord_bot_token_here

# OpenRouter API Key (for LLM functionality)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# Web Search Configuration
# Choose search provider: 'google' or 'tavily' (default: tavily)
SEARCH_PROVIDER=tavily

# Tavily API Key (for web search - default provider)
TAVILY_API_KEY=your_tavily_api_key_here

# Google Custom Search API (optional - higher quality results, 100 free queries/day)
# Get API key: https://developers.google.com/custom-search/v1/overview
# Create search engine: https://programmablesearchengine.google.com/
# GOOGLE_SEARCH_API_KEY=your_google_api_key_here
# GOOGLE_SEARCH_CX=your_custom_search_engine_id_here

# Wolfram Alpha API (optional - for calculations, conversions, and computational knowledge)
# Get App ID: https://developer.wolframalpha.com/
# Free tier: 2,000 queries/month
# WOLFRAM_APP_ID=your_wolfram_app_id_here

# OpenWeatherMap API (optional - for weather information)
# Get API key: https://openweathermap.org/api
# Free tier: 1,000 calls/day, 60 calls/minute
# OPENWEATHER_API_KEY=your_openweather_api_key_here

# LLM Configuration
MODEL_NAME=deepseek/deepseek-chat  # Fast, cost-effective model for general chat
FACT_CHECK_MODEL=deepseek/deepseek-chat  # Model for fact-checking
CONTEXT_WINDOW_MESSAGES=50  # Number of recent messages to include in context (with compression)

# Conversation Compression (LLMLingua)
# Reduces token usage by 50-80% using semantic compression
# Allows longer conversation history within API limits
ENABLE_COMPRESSION=true  # Enable/disable conversation compression
COMPRESSION_RATE=0.5  # Target compression rate (0.5 = 50% token reduction)
MIN_MESSAGES_TO_COMPRESS=8  # Minimum messages needed before compression activates
COMPRESSION_MODEL=microsoft/llmlingua-2-bert-base-multilingual-cased  # Compression model (smaller/faster than xlm-roberta-large)

# Local Uncensored LLM Configuration (Optional - for /uncensored command)
# Set LOCAL_LLM_ENABLED=true to enable local model routing
# Supports Ollama, LM Studio, vLLM, or any OpenAI-compatible API
LOCAL_LLM_ENABLED=false
LOCAL_LLM_URL=http://localhost:11434/v1  # Default Ollama endpoint
LOCAL_LLM_MODEL=dolphin-llama3:latest  # Drummer's dolphin models are popular uncensored options
LOCAL_LLM_TIMEOUT=60  # Timeout in seconds (local models can be slower)
# LOCAL_LLM_API_KEY=your_key_here  # Only needed if your local server requires auth

# Privacy Settings
OPT_OUT_ROLE_NAME=AI-Opt-Out

# Admin Configuration
# Bot admin user IDs (comma-separated) - these users bypass rate limits
# BOT_ADMIN_IDS=123456789012345678,234567890123456789
# Admin username (legacy - prefer BOT_ADMIN_IDS)
# BOT_ADMIN_USERNAME=your_discord_username

# Command Sync Configuration
# Primary guild ID for slash command syncing (faster sync during development)
# Leave empty to sync globally (slower but works across all servers)
# PRIMARY_GUILD_ID=your_guild_id_here

# Database Configuration
POSTGRES_PASSWORD=your_secure_database_password_here

# SECURITY NOTES:
# 1. Keep .env file LOCAL ONLY - never commit to git
# 2. Use strong, unique passwords for POSTGRES_PASSWORD
# 3. Rotate tokens/keys if accidentally exposed
# 4. Set restrictive file permissions: chmod 600 .env
